# 算法组小萌新线性代数快速入门指南
*by: MicDZ [micdz@hust.edu.cn](mailto:micdz@hust.edu.cn?subject="Robomaster线性代数快速入门指南咨询")*
[https://github.com/MicDZ/linear-algebra-quick-start/](https://github.com/MicDZ/linear-algebra-quick-start/)
## 前言

大一新生在没有学习线性代数的情况下入门算法组存在障碍。从头到尾系统学习线性代数需要较长时间。本文期望解决这一痛点。

- 本文面向的对象有：大一新生、对线性代数实质不太理解的高年级同学。

- 本文讲解的重点内容有:
	1. 线性代数的基本概念内涵
	2. 线性代数的实质
	3. 线性代数在工程中的一般应用

- 本文**不会**讲解的内容有：
	1. 线性代数的概念的具体定义
	2. 线性代数性质定理的证明
	3. 线性代数的例题
	4. 矩阵论的高级内容

注意：本文将不会是一篇指导完成课业内容的文章。本文更注重立足于实践。旨在帮助你真正“会用”线性代数。


## 向量

高中我们都学过向量。我们用向量来表示一个有大小有方向的量。过去，我们用一个“数对”表示**向量**。
$$
\begin{align}
\vec{a}=&(x_1,y_1)\quad \vec b=(x_2,y_2)\\
|\vec a|&=\sqrt{x_1^2+y_1^2}\\
\vec a\cdot\vec b&=x_1x_2+y_1y_2\\
\cos\theta&=\frac{\vec a\cdot\vec b}{||\vec a||\ ||\vec b||}
\end{align}
$$
与高中不同，现在我们通常用“列”来表示向量（也有行向量，为方便后文均采用列向量）。
例如，在线性代数中，我们将二维的列向量理解为一个两行一列的“矩阵”（矩阵的概念在下节介绍）。向量的模的表示也有所区别，你需要习惯这种表示方法。
$$
\begin{align}
\vec a=&\begin{bmatrix}x_1\\y_1\end{bmatrix}\quad
\vec b=\begin{bmatrix}x_2\\y_2\end{bmatrix}\\
||\vec a||&=\sqrt{x_1^2+y_1^2}
\end{align}
$$
## 线性组合

从这里开始算是正式进入了线性代数的学习。很明显，“线性”是线性代数研究的关键。我们首先介绍一下什么是“线性”。
线性的最直观理解就是一条直线。
$$
y=kx+b
$$


我们引入线性组合的概念。设有两条直线：
$$
\begin{align}
y=&kx+b\dots\ (1)\\
y=&mx+n\dots(2)
\end{align}
$$
我们将两条直线进行任意的“线性组合”，很容易发现，所得结果仍为一条线性直线。
$$
(1)\times c+(2)\times d
$$
$$
\begin{align}

y&=c(kx+b)+d(mx+n)\\
&=(ck+dm)x+(cb+dn)
\end{align}
$$
其实这种特性可以用“线性运算封闭性”高度概括。换言之，将直线方程乘一个数，或者将两条直线方程相加，所得结果仍为一条直线。这是理解后续“线性空间”的重要基础。

如果将上述过程用我们刚才所学的向量表示，那么可以写成（暂时理解为一种记号，但其实有其几何含义）：
$$
\begin{align}
\vec u =\begin{bmatrix}k\\b\end{bmatrix}\quad \vec w=\begin{bmatrix}m\\n\end{bmatrix}\\
c\vec u+d\vec w=\begin{bmatrix}ck+dm\\cb+dn\end{bmatrix}
\end{align}
$$
我们就将 $c\vec u+d\vec w$ 称为向量之间的**线性组合**。这里，我们也可以发现**二维**向量之间的线性组合所得结果仍然可以是一个**二维**向量。

如果我们将情况拓展到三维。看下面一个例子：

$$
\vec u=\begin{bmatrix}1\\0\\0\end{bmatrix}
\quad
\vec v=\begin{bmatrix}0\\1\\0\end{bmatrix}
\quad
\vec w=\begin{bmatrix}0\\0\\1\end{bmatrix}
$$
通过这三个向量的线性组合，我们可以得到整个三维空间内的所有向量。因为对于任意一个三维向量我们都可以拆分到这三个分量上。
$$
\begin{bmatrix}a\\b\\c\end{bmatrix}=a\vec u+b\vec v+c\vec w
$$
但是，是否任意给定三个向量都可实现通过三个向量之间的线性组合得到三维空间内的所有向量呢？请你先思考一下，这将会在后面介绍。
$$
\vec u=\begin{bmatrix}1\\0\\0\end{bmatrix}
\quad
\vec v=\begin{bmatrix}0\\1\\0\end{bmatrix}
\quad
\vec w=\begin{bmatrix}0\\2\\0\end{bmatrix}
$$

## 矩阵
矩阵是线性代数最基础的概念。后面我们所有的讨论也围绕矩阵展开，但你不用太担心，现在你需要做的是将矩阵看做一个“记号”，其本质和xyz这种变量没有区别。

假定我现在有三个向量，我需要对他们进行线性组合。
$$
\vec u=\begin{bmatrix}1\\-1\\0\end{bmatrix}\quad
\vec v=\begin{bmatrix}0\\1\\-1\end{bmatrix}\quad
\vec w=\begin{bmatrix}0\\0\\1\end{bmatrix}
$$

$$
x_1\vec u+x_2\vec v+x_3\vec w=\begin{bmatrix}x_1\\x_2-x_1\\x_3-x_2\end{bmatrix}
$$
$x_1\vec u+x_2\vec v+x_3\vec w$这个写法未免太复杂，我们用“矩阵”将这个式子改写一下：
$$
A\mathbf x=\begin{bmatrix}1 & 0 & 0\\-1 & 1 & 0\\0 & -1 & 1\end{bmatrix}\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}=\begin{bmatrix}x_1\\x_2-x_1\\x_3-x_2\end{bmatrix}=\begin{bmatrix}b_1\\b_2\\b_3\end{bmatrix}=\mathbf b
$$

这里用了**矩阵的乘法**，其本质是“行列（点）积”，也就是将前一个矩阵中的每一个行向量与后一个矩阵中的列向量求点积，然后将点积所得结果放在对应的位置。

在这里因为后面的矩阵只有一个列向量，那么就可以将矩阵乘法这么理解
$$
A\mathbf x=\begin{bmatrix}1 & 0 & 0\\-1 & 1 & 0\\0 & -1 & 1\end{bmatrix}\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}=\begin{bmatrix}(1,0,0)\cdot(x_1,x_2,x_3)\\(-1,1,0)\cdot(x_1,x_2,x_3)\\(0,-1,1)\cdot(x_1,x_2,x_3)\end{bmatrix}
$$

那么就可以得到一个非常简单的写法，我们将它称为**线性方程组**：
$$
A\mathbf x=\mathbf b
$$
如果我们已知 $A$ 和 $\mathbf b$ ，想要求得 $\mathbf x$  怎么做呢？看上去很简单，只需要

$$
\mathbf x=A^{-1}\mathbf b
$$
这里 $A^{-1}$ 就被称为**矩阵的逆**。这里也只是一个记号，并不代表实际就是求“负一次幂”，实际计算参考教材。

但实际情况并不会这么简单，$\mathbf x$ 是否有解必然是与 $A$ 和 $\mathbf b$ 有关的。下面一节我们讨论这种相关性。

## 线性独立与线性相关

还是考虑刚才的例子，我们再添加一个向量，记为 $\vec{w^*}$ 。
$$
\vec u=\begin{bmatrix}1\\-1\\0\end{bmatrix}\quad
\vec v=\begin{bmatrix}0\\1\\-1\end{bmatrix}\quad
\vec w=\begin{bmatrix}0\\0\\1\end{bmatrix}\quad 
\vec{w^*}=\begin{bmatrix}-1\\0\\1\end{bmatrix}
$$
我们可以发现，$\vec{w^*}=-\vec u-\vec v$ ，也就是说 $\vec{w^*}$ 可以由 $\vec u$ 和 $\vec v$ 线性组合得来。那么我们称 $\vec{w^*},\vec u,\vec v$ **线性相关**。相反地 $\vec{w}$ 不可以由 $\vec u$ 和 $\vec v$ 线性组合得来，那么我们称 $\vec{w},\vec u,\vec v$ **线性独立**。

我们在这里尝试在空间中理解这个事实。
![Pasted image 20231015142207.png](./img/Pasted%20image%2020231015142207.png)

$\vec u,\vec v,\vec w$ 线性独立，在空间中的具体体现就是 $\vec w,\vec u,\vec v$ 不会在同一个平面内。
![Pasted image 20231015142417.png](./img/Pasted%20image%2020231015142417.png)
而$\vec{w^*},\vec u,\vec v$ 线性相关，那么这三个向量将会在同一个平面上。

现在我们考虑这个方程的解
$$
A\mathbf x=\mathbf b
$$
$A$ 表示的就是 $\vec u,\vec v,\vec w$ 这一组向量。 $A\mathbf x$ 就是将这一组向量进行线性组合。

显然，对于 $\vec u,\vec v,\vec w$ 这一组向量，无论 $\mathbf b$ 落在空间内的哪一个点，都可以解出对应的 $\mathbf x$ ；而对于 $\vec u,\vec v,\vec{w^*}$ 这一组向量，只有当 $\mathbf b$ 落在他们所张成的平面上时， $\mathbf x$ 才会有解。

## 向量空间

还是以刚才四个向量为例
$$
\vec u=\begin{bmatrix}1\\-1\\0\end{bmatrix}\quad
\vec v=\begin{bmatrix}0\\1\\-1\end{bmatrix}\quad
\vec w=\begin{bmatrix}0\\0\\1\end{bmatrix}\quad 
\vec{w^*}=\begin{bmatrix}-1\\0\\1\end{bmatrix}
$$
$\vec u,\vec v,\vec w$ 这一组向量，张成了一个三维的空间，那么我们就称这个三维空间为三维的**向量空间**，记为 $C(A)=\mathbf R^3$ 。同理 $\vec u,\vec v,\vec{w^*}$ 这一组向量，张成了一个二维的空间，我们称为二维的向量空间，记为 $\mathbf R^2$ 。

那么我们将刚才的可解性问题转化一下术语，就可以这样表述：

$A\mathbf x=\mathbf b$ 当且仅当 $\mathbf b$ 属于 $A$  的向量空间内时有解。（需要自己理解一下什么叫做“属于”）

这里我们就可以将**矩阵的秩**有一个直观的理解了，矩阵的秩就是组成矩阵的向量组所张成的向量空间的维数。

### 零空间

上一节，我们能够直观理解线性方程组有解和无解的条件，但是究竟如何解这个方程呢，我们从这一小节开始讨论。

为了解 $A\mathbf x=\mathbf b$ 这一方程，我们引入了“零空间”的概念。简单来说，**零空间**是 $A\mathbf x=\mathbf 0$ 的解空间。为什么叫做零“空间”呢？
假设有一个解 $\mathbf {x}$ 满足 $A\mathbf x=\mathbf 0$，那么对 $\mathbf x$ 点乘任何数有 $A(c\mathbf x)=cA\mathbf x=c\mathbf 0=\mathbf 0$ ，仍为 $\mathbf 0$。也就是说，任何满足 $A\mathbf x=\mathbf 0$ 的解经过线性组合后，仍为这个方程的解。我们设其中一个解为 $\mathbf x_n$ 。记这个空间为 $N(A)$ 。

同样的对于 $A\mathbf x=\mathbf b$ 这个方程，如果有某一个解 $\mathbf x_p$，那么 $\mathbf x_p+\mathbf x_n$ 也一定是其解。

那么我们就可以用 $\mathbf x_p+k\mathbf x_n$ 表示 $A\mathbf x=\mathbf b$ 的所有解。可以证明所有的解都可用这个式子表示。

### 转置后的空间
把 $m\times n$ 矩阵A的行换成同序数的列得到一个 $n\times m$ 矩阵，此矩阵叫做A的转置矩阵，记为 $A^\top$ 。

例如
$$
A=\begin{bmatrix}1 & 2 & 0\\3 & -1 & 4\end{bmatrix}\quad
A^\top=\begin{bmatrix}1 & 3 \\ 2 & -1 \\ 0 & 4\end{bmatrix}
$$

我们就有了四个空间：
1. $\text C(A)$：行空间
2. $\text C(A^\top)$：列空间
3. $N(A)$ ：零空间
4. $N(A^\top)$ ：左零空间 
这四个空间之间显然存在一些相关关系，期望读者自行学习一下。
### 正交性

对于两个向量而言
$$
\begin{align}
\mathbf u =\begin{bmatrix}1\\-1\end{bmatrix}\quad&\mathbf v=\begin{bmatrix}-1\\1\end{bmatrix}\\
\mathbf u^\top\mathbf v&=\mathbf 0
\end{align}
$$
利用矩阵乘法的性质，我们可以用 $\mathbf u^\top\mathbf v$ 表示两个向量的点乘，点乘的值为 $\mathbf 0$ 那么两个向量就正交。


如果是两个矩阵正交：可以采用同样的定义 $\mathbf A^\top\mathbf B=\mathbf 0$ 。

线性代数的理论部分大致就到此结束。

## 行列式

很多线性代数开篇就是介绍行列式的计算，但行列式本身的意义其实必须要在能够理解向量空间后能够理解。所以本文将行列式的介绍放在了一个较后的位置。

对于一个 $n\times n$ 的矩阵，行列式的几何含义是：
定义在n维空间下一组单位正交基所构成的体积为1。这个矩阵的行列式就等于，这个n维矩阵的列（或行）向量组所张成的体积。

如果是一个 $3\times 3$ 的矩阵：
$$
\begin{bmatrix}1 & 0 & 0\\0 & 2 & 0\\0 & 0 &1\end{bmatrix}
$$
这一组列向量显然是将y轴拉伸了2倍，张成的体积就是标准单位正交基张成空间的两倍。自然其行列式的值就是2。



## 线性变换

在正式开始讲具体的变换之前，希望读者能从宏观上对“变换”有一个理解。我们往往将变换称为“映射”，对应的是将一个集合映射到另一个集合。函数就是一种最简单的变换。后文我们要讨论的变换也与此类似。

我们暂时先用 $\text T$ 来表示这种变换，其实矩阵乘法就是一种映射（想想为什么）。线性变换对变换本身有要求，与此前的线性运算封闭非常类似

$$
\begin{align}
\text T(\mathbf u+\mathbf v) &= \text T(\mathbf u)+\text T(\mathbf v)\\
\text T(c\mathbf u) &=c\text T(\mathbf u)
\end{align}
$$

### 平移

平移明显不是一个线性变换（想想为什么）。
平移可以直接利用向量的加法实现
$$
\text T(\mathbf w)=\mathbf w+\mathbf u_0
$$

### 旋转
![Pasted image 20231015162403.png](./img/Pasted%20image%2020231015162403.png)
两个向量的点积在被旋转后保持不变。记旋转矩阵为 $\mathbf M$ 。
$$
\mathbf a^\top\cdot\mathbf b = (\mathbf M\mathbf a)^\top\cdot\mathbf M\mathbf b=\mathbf a^\top(\mathbf M^\top \mathbf M\mathbf) b 
$$
所以可得出
$$
\mathbf M\mathbf M^\top=\mathbf I 
$$

TODO

### 投影

TODO

## 参考资料

(1). 一本很好的入门教材：[Strang G .Introduction to Linear Algebra[J].Wellesley-Cambridge Press, 2003.DOI:10.1007/978-1-4612-1070-2.](https://math.mit.edu/~gs/linearalgebra/) 
(2). 主要讲线性代数应用的教材：Strang G. Linear algebra and its applications[M]. 2012.MLA
(3). 直观图解线性代数的本质：[https://www.bilibili.com/video/BV1Ys411k7yQ/](https://www.bilibili.com/video/BV1Ys411k7yQ/
(4). 参考资料(1)的配套网课：[https://www.bilibili.com/video/BV16Z4y1U7oU](https://www.bilibili.com/video/BV16Z4y1U7oU)
